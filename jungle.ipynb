{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "from sklearn import svm\n",
    "from scipy import sparse\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load(k=2), load(X=False, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X, Z):\n",
    "    return (X[:, None, :] == Z[None, :, :]).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=kernel)\n",
    "evaluate(clf, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrum Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black magic function to extract subsequences as views of the source array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    \"\"\"\n",
    "    Make an ndarray with a rolling window of the last dimension\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array_like\n",
    "       Array to add rolling window to\n",
    "    window : int\n",
    "       Size of rolling window\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Array that is a view of the original array with a added dimension\n",
    "    of size w.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> x=np.arange(10).reshape((2,5))\n",
    "    >>> rolling_window(x, 3)\n",
    "    array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],\n",
    "          [[5, 6, 7], [6, 7, 8], [7, 8, 9]]])\n",
    "\n",
    "    Calculate rolling mean of last dimension:\n",
    "    >>> np.mean(rolling_window(x, 3), -1)\n",
    "    array([[ 1.,  2.,  3.],\n",
    "          [ 6.,  7.,  8.]])\n",
    "\n",
    "    \"\"\"\n",
    "    if window < 1:\n",
    "        raise ValueError(\"`window` must be at least 1.\")\n",
    "    if window > a.shape[-1]:\n",
    "        raise ValueError(\"`window` is too long.\")\n",
    "        \n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    \n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_prod(a, b, n, m):\n",
    "    inds_a, counts_a = a\n",
    "    inds_b, counts_b = b\n",
    "    \n",
    "    kernel = np.zeros((n, m))\n",
    "    table_a = dict()\n",
    "    \n",
    "    last_i = -1\n",
    "    xs = []\n",
    "    counts = []\n",
    "    \n",
    "    for (i, x), count in zip(inds_a, counts_a):\n",
    "        if i != last_i:\n",
    "            table_a[last_i] = (xs, counts)\n",
    "            \n",
    "            xs = []\n",
    "            counts = []\n",
    "            last_i = i\n",
    "        \n",
    "        xs.append(int(x))\n",
    "        counts.append(count)\n",
    "    \n",
    "    table_a[last_i] = (xs, counts)\n",
    "    \n",
    "    for (j, y), count in zip(inds_b, counts_b):\n",
    "        if j not in table_a:\n",
    "            continue\n",
    "        xs, counts = table_a[j]\n",
    "\n",
    "        kernel[xs, int(y)] += np.asarray(counts)*count\n",
    "    \n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_grams(X, k=2, m=4, ret_inds=False):\n",
    "    N, L = X.shape\n",
    "    \n",
    "    encoding = (m ** np.arange(k, dtype=int)).astype(np.uint64)\n",
    "    \n",
    "    k_grams_indices = rolling_window(X, window=k).dot(encoding)\n",
    "    \n",
    "    xs = np.repeat(np.arange(N), repeats=k_grams_indices.shape[1])\n",
    "    ys = k_grams_indices.reshape(-1)\n",
    "    \n",
    "    inds = np.stack((xs, ys), axis=1)\n",
    "    inds, counts = np.unique(inds, axis=0, return_counts=True)\n",
    "    \n",
    "    res = sparse.csr_matrix((counts, (inds[:, 0], inds[:, 1])), shape=(N, m ** k), dtype=int)\n",
    "    if ret_inds:\n",
    "        return res, np.unique(np.stack((ys, xs), axis=1), axis=0, return_counts=True)\n",
    "    return res\n",
    "\n",
    "\n",
    "def k_spectrum(X, Y=None, k=2, m=4):\n",
    "    \"\"\"\n",
    "     Computes the k-spectrum kernel between the sequences from X and Y.\n",
    "     If Y is None, computes the k-spectrum between sequences from X.\n",
    "     \n",
    "     NB: 'normalizes' the kernel, but does not center it.\n",
    "    \"\"\"\n",
    "    if k > 13:\n",
    "        return k_spectrum_extreme(X, Y, k, m)\n",
    "    \n",
    "    k_grams_X = k_grams(X, k=k, m=m)\n",
    "    k_grams_Y = k_grams_X if Y is None else k_grams(Y, k=k, m=m)\n",
    "    \n",
    "    # TODO: Normalization? Centering?\n",
    "    norms_X = sparse.linalg.norm(k_grams_X, axis=1)\n",
    "    norms_Y = sparse.linalg.norm(k_grams_Y, axis=1)\n",
    "    \n",
    "    K = k_grams_X.dot(k_grams_Y.T).toarray() / np.outer(norms_X, norms_Y)\n",
    "    \n",
    "    return K\n",
    "\n",
    "\n",
    "def k_spectrum_extreme(X, Y=None, k=2, m=4):\n",
    "    k_grams_X, k_grams_inds_X = k_grams(X, k=k, m=m, ret_inds=True)\n",
    "    k_grams_Y, k_grams_inds_Y = (k_grams_X, k_grams_inds_X) if Y is None else k_grams(Y, k=k, m=m, ret_inds=True)\n",
    "    \n",
    "    K = sparse_prod(k_grams_inds_X, k_grams_inds_Y, len(X), len(X) if Y is None else len(Y))\n",
    "    \n",
    "    # TODO: Normalization? Centering?\n",
    "    norms_X = sparse.linalg.norm(k_grams_X, axis=1)\n",
    "    norms_Y = sparse.linalg.norm(k_grams_Y, axis=1)\n",
    "    \n",
    "    K /= np.outer(norms_X, norms_Y)\n",
    "    \n",
    "    return K\n",
    "\n",
    "def cum_spectrum(X, Y=None, k=5, tqdm=False):\n",
    "    \"\"\"\n",
    "     Computes the sum of the spectrum kernels between X and Y, up to k.\n",
    "    \"\"\"\n",
    "    shape = (len(X), len(X) if Y is None else len(Y))\n",
    "    kernel = np.zeros(shape)\n",
    "    \n",
    "    for i in (tqdm_notebook(range(1, k+1)) if tqdm else range(1, k+1)):\n",
    "        kernel += k_spectrum(X, Y=Y, k=i)\n",
    "    \n",
    "    return kernel / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds_indices(n, k):\n",
    "    \"\"\"\n",
    "     Return k pairs (train_inds, valid_inds) of arrays containing the training and validation indices for each split.\n",
    "    \"\"\"\n",
    "    assert(n % k == 0)\n",
    "    m = n // k\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append((np.concatenate((indices[:i*m], indices[(i+1)*m:])),\n",
    "                      indices[i*m:(i+1)*m]))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(classifier, K, Y, folds=5):\n",
    "    \"\"\"\n",
    "    :param classifier: classifier to evaluate\n",
    "    :param K: precomputed kernel matrix of shape (n_samples, n_samples)\n",
    "    :param Y: training labels of shape (n_samples, )\n",
    "    :param folds: number of folds to use\n",
    "    \n",
    "    Evaluates the classifier using cross-validation.\n",
    "    Returns the mean and std of the validation and train scores.:\n",
    "        (valid_scores_mean, valid_scores_std, train_scores_mean, train_scores_std)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(K)\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    for train_inds, valid_inds in k_folds_indices(N, folds):\n",
    "        classifier.fit(K[np.ix_(train_inds, train_inds)], Y[train_inds])\n",
    "        \n",
    "        valid_scores.append((classifier.predict(K[np.ix_(valid_inds, train_inds)]) == Y[valid_inds]).mean())\n",
    "        train_scores.append((classifier.predict(K[np.ix_(train_inds, train_inds)]) == Y[train_inds]).mean())\n",
    "        \n",
    "    valid_scores = np.array(valid_scores)\n",
    "    train_scores = np.array(train_scores)\n",
    "    \n",
    "    return valid_scores.mean(), valid_scores.std(), train_scores.mean(), train_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_evaluate(classifier, Ks, Ys, Cs, folds=5, **params):\n",
    "    \"\"\"\n",
    "    :param classifier: classifier to evaluate\n",
    "    :param Ks: list precomputed kernel matrices of shape list(n_samples_i, n_samples_i)\n",
    "    :param Ys: training labels of shape list(n_samples_i,)\n",
    "    :param Cs: list of parameters C\n",
    "    :param folds: number of folds to use\n",
    "    :param params: additional parameters to instantiate the classifier\n",
    "    \n",
    "    Evaluates a classifier on several data sets, and averages the results.\n",
    "    \"\"\"\n",
    "    return np.mean(np.array([evaluate(classifier(C=C, **params), K, Y, folds) for (K, Y, C) in zip(Ks, Ys, Cs)]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(K, Y, folds=5, C_min=-7, C_max=6):\n",
    "    \"\"\"\n",
    "     Grid search on C values, from 10^C_min to 10^C_max, using k-fold cross validation.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for C in 10. ** np.arange(C_min, C_max + 1):\n",
    "        results.append(np.array(evaluate(svm.SVC(kernel='precomputed', C=C), K, Y, folds=folds)))\n",
    "        print('%.0e \\t ' % C,\n",
    "              'Validation %.2f%% +- %.2f\\t Train %.2f%%\\t +- %.2f' %\n",
    "              tuple(100 * results[-1]))\n",
    "    \n",
    "    return 10. ** np.arange(C_min, C_max + 1), results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, train_Xs, train_Ys, test_Xs, Cs, file_name='predictions', **params):\n",
    "    \"\"\"\n",
    "    For each data set, trains the model on the train data, then computes predictions on the test data.\n",
    "    Saves the predictions to the specified file.\n",
    "    \n",
    "    Also evaluates the performance of the specified model using cross-validation on the train data.\n",
    "    \n",
    "    NB: train_Xs and test_Xs can be either precomputed kernel matrices, or simply the train and test features,\n",
    "    depending on the model and parameters.\n",
    "    \"\"\"\n",
    "    # Evaluation using k-fold validation.\n",
    "    print('Evaluation on train data (using cross validation)')\n",
    "    print('Validation %.2f\\%% +- %.2f\\t Train %.2f%%\\t +- %.2f' %\n",
    "          tuple(100 * global_evaluate(model, train_Xs, train_Ys, Cs, **params)))\n",
    "    predictions = []\n",
    "    for k, (X_train, Y_train, X_test, C) in enumerate(zip(train_Xs, train_Ys, test_Xs, Cs)):\n",
    "        model_k = model(C=C, **params)\n",
    "        model_k.fit(X_train, Y_train)\n",
    "        \n",
    "        predictions.append(model_k.predict(X_test))\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    \n",
    "    predictions_dir = 'predictions'\n",
    "    if not os.path.isdir(predictions_dir):\n",
    "        os.mkdir(predictions_dir)\n",
    "    \n",
    "    np.savetxt('%s/%s.csv' % (predictions_dir, file_name),\n",
    "               np.stack((np.arange(len(predictions)),\n",
    "                         predictions), axis=1),\n",
    "               header='Id,Bound', comments='', fmt='%d', delimiter=',')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline using Spectrum kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computations are slowed down over $k=13$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_k = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xs = [load(k=k) for k in range(3)]\n",
    "train_Ys = [load(X=False, k=k) for k in range(3)]\n",
    "test_Xs = [load(k=k, train=False) for k in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tqdm = True\n",
    "\n",
    "train_Ks = [cum_spectrum(train_X, k=spectrum_k, tqdm=tqdm) for train_X in train_Xs]\n",
    "test_Ks  = [cum_spectrum(test_X, train_X, k=spectrum_k, tqdm=tqdm) for (test_X, train_X) in zip(test_Xs, train_Xs)]\n",
    "\n",
    "print(\"Total Duration: %.1f seconds.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search to find the best values for parameter C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs0 = grid_search(train_Ks[0], train_Ys[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs1 = grid_search(train_Ks[1], train_Ys[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = grid_search(train_Ks[2], train_Ys[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_Cs = [Cs[np.argmax(np.array(scores)[:, 0] - np.array(scores)[:, 1])]\n",
    "             for (Cs, scores) in [gs0, gs1, gs2]]  # Values with best validation score.\n",
    "\n",
    "Cs = (1, 1, 1)               # Values without strong overfitting (i.e. perfect training accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy values of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test(svm.SVC,\n",
    "         train_Ks, train_Ys, test_Ks,\n",
    "         Cs=greedy_Cs, kernel='precomputed',\n",
    "         file_name='spectrum_greedy_predictions_%d' % spectrum_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft values of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test(svm.SVC,\n",
    "         train_Ks, train_Ys, test_Ks,\n",
    "         Cs=Cs, kernel='precomputed',\n",
    "         file_name='spectrum_soft_predictions_%d' % spectrum_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
